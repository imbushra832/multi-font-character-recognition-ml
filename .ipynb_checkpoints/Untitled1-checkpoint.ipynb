{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8f146a-624a-463f-9261-5368b2156bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# Custom RBF Network implementation\n",
    "class RBFNetwork(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_centers=52, sigma=1.0, learning_rate=0.01, epochs=100, batch_size=10, \n",
    "                 initialization='kmeans', center_learning_rate=0.01):\n",
    "        self.n_centers = n_centers\n",
    "        self.sigma = sigma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.initialization = initialization\n",
    "        self.center_learning_rate = center_learning_rate\n",
    "        self.centers = None\n",
    "        self.weights = None\n",
    "        self.classes_ = None\n",
    "        self.n_classes = None\n",
    "        self.sigmas = None\n",
    "        \n",
    "    def _initialize_centers(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if self.initialization == 'random':\n",
    "            # Random initialization\n",
    "            idx = np.random.choice(n_samples, self.n_centers, replace=False)\n",
    "            self.centers = X[idx]\n",
    "            \n",
    "        elif self.initialization == 'kmeans':\n",
    "            # Use K-means for center initialization\n",
    "            from sklearn.cluster import KMeans\n",
    "            kmeans = KMeans(n_clusters=self.n_centers, random_state=42)\n",
    "            kmeans.fit(X)\n",
    "            self.centers = kmeans.cluster_centers_\n",
    "            \n",
    "        elif self.initialization == 'per_class':\n",
    "            # Initialize centers from each class\n",
    "            centers_per_class = max(1, self.n_centers // self.n_classes)\n",
    "            centers = []\n",
    "            \n",
    "            for i in range(self.n_classes):\n",
    "                class_samples = X[y == i]\n",
    "                if len(class_samples) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                if len(class_samples) < centers_per_class:\n",
    "                    # If we have fewer samples than required centers, use all samples\n",
    "                    centers.append(class_samples)\n",
    "                else:\n",
    "                    # Randomly select centers_per_class samples\n",
    "                    idx = np.random.choice(len(class_samples), centers_per_class, replace=False)\n",
    "                    centers.append(class_samples[idx])\n",
    "                    \n",
    "            self.centers = np.vstack(centers)\n",
    "            if len(self.centers) < self.n_centers:\n",
    "                # If we couldn't get enough centers, fill with random samples\n",
    "                remaining = self.n_centers - len(self.centers)\n",
    "                idx = np.random.choice(n_samples, remaining, replace=False)\n",
    "                self.centers = np.vstack([self.centers, X[idx]])\n",
    "        \n",
    "        # Initialize sigmas (widths) for each center\n",
    "        # Start with a constant sigma for all centers\n",
    "        self.sigmas = np.ones(self.n_centers) * self.sigma\n",
    "        \n",
    "    def _compute_rbf_outputs(self, X):\n",
    "        # Compute RBF activations for each center\n",
    "        # Shape: (n_samples, n_centers)\n",
    "        distances = cdist(X, self.centers)\n",
    "        # Apply RBF function using individual sigmas for each center\n",
    "        return np.exp(-0.5 * np.square(distances) / np.square(self.sigmas.reshape(1, -1)))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Convert y to numerical if needed\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes = len(self.classes_)\n",
    "        y_numeric = np.zeros(len(y), dtype=int)\n",
    "        for i, cls in enumerate(self.classes_):\n",
    "            y_numeric[y == cls] = i\n",
    "        \n",
    "        # One-hot encode the target\n",
    "        y_onehot = np.zeros((len(y), self.n_classes))\n",
    "        for i in range(len(y)):\n",
    "            y_onehot[i, y_numeric[i]] = 1\n",
    "        \n",
    "        # Initialize centers\n",
    "        self._initialize_centers(X, y_numeric)\n",
    "        \n",
    "        # Initialize weights (n_centers x n_classes)\n",
    "        self.weights = np.random.normal(0, 0.1, (self.n_centers, self.n_classes))\n",
    "        \n",
    "        # Training loop\n",
    "        n_samples = X.shape[0]\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "            y_onehot_shuffled = y_onehot[indices]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                end = min(i + self.batch_size, n_samples)\n",
    "                X_batch = X_shuffled[i:end]\n",
    "                y_batch = y_onehot_shuffled[i:end]\n",
    "                \n",
    "                # Forward pass\n",
    "                rbf_outputs = self._compute_rbf_outputs(X_batch)\n",
    "                y_pred = np.dot(rbf_outputs, self.weights)\n",
    "                \n",
    "                # Compute error and deltas\n",
    "                error = y_batch - y_pred\n",
    "                weight_deltas = self.learning_rate * np.dot(rbf_outputs.T, error)\n",
    "                \n",
    "                # Update weights\n",
    "                self.weights += weight_deltas\n",
    "                \n",
    "                # Two-phase learning: Update centers if needed\n",
    "                if self.center_learning_rate > 0:\n",
    "                    # Compute the contribution of each center to the error\n",
    "                    center_deltas = np.zeros_like(self.centers)\n",
    "                    sigma_deltas = np.zeros_like(self.sigmas)\n",
    "                    \n",
    "                    for j in range(self.n_centers):\n",
    "                        # For each center, calculate how it affects the output error\n",
    "                        for k in range(len(X_batch)):\n",
    "                            # Compute the influence of this center on the output\n",
    "                            rbf_value = rbf_outputs[k, j]\n",
    "                            delta = np.sum(error[k] * self.weights[j])\n",
    "                            \n",
    "                            # Gradient for center update\n",
    "                            diff = X_batch[k] - self.centers[j]\n",
    "                            center_gradient = delta * rbf_value * diff / (self.sigmas[j] ** 2)\n",
    "                            center_deltas[j] += center_gradient\n",
    "                            \n",
    "                            # Gradient for sigma update\n",
    "                            dist_squared = np.sum(np.square(diff))\n",
    "                            sigma_gradient = delta * rbf_value * dist_squared / (self.sigmas[j] ** 3)\n",
    "                            sigma_deltas[j] += sigma_gradient\n",
    "                    \n",
    "                    # Update centers and sigmas\n",
    "                    self.centers += self.center_learning_rate * center_deltas / len(X_batch)\n",
    "                    self.sigmas += self.center_learning_rate * sigma_deltas / len(X_batch)\n",
    "                    \n",
    "            # Optional: Add validation to monitor progress and early stopping\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        rbf_outputs = self._compute_rbf_outputs(X)\n",
    "        raw_output = np.dot(rbf_outputs, self.weights)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        exp_scores = np.exp(raw_output - np.max(raw_output, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(proba, axis=1)]\n",
    "\n",
    "\n",
    "# Function to load and prepare data\n",
    "def load_data(train_data_path='train_data.txt', test_data_path='test_data.txt'):\n",
    "    \"\"\"\n",
    "    Load and preprocess the multi-font character recognition data.\n",
    "    \n",
    "    Returns:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "    \"\"\"\n",
    "    # Manually construct the dataset as per provided format\n",
    "    # For demonstration, I'll generate sample data based on the description\n",
    "    # In practice, you would load from the text files\n",
    "    \n",
    "    # Function to parse the data from text files\n",
    "    def parse_data_file(file_path):\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            # Skip non-data lines\n",
    "            if len(lines[i].strip()) < 5 or \"Pattern\" in lines[i]:\n",
    "                i += 1\n",
    "                continue\n",
    "                \n",
    "            # Find line starting with \"Inputs Outputs\" or containing data\n",
    "            if \"Inputs Outputs\" in lines[i]:\n",
    "                i += 1\n",
    "                continue\n",
    "                \n",
    "            # Try to parse the data line\n",
    "            try:\n",
    "                parts = lines[i].strip().split()\n",
    "                if len(parts) >= 40:  # Expect 14 inputs + 26 outputs\n",
    "                    inputs = [float(val) for val in parts[:14]]\n",
    "                    outputs = [int(val) for val in parts[14:40]]\n",
    "                    \n",
    "                    X.append(inputs)\n",
    "                    # Convert one-hot to class index (0-25 for A-Z)\n",
    "                    y.append(np.argmax(outputs))\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    try:\n",
    "        # Try to load from the actual files\n",
    "        X_train, y_train = parse_data_file('train_data_3.txt')\n",
    "        X_test, y_test = parse_data_file('test_data_3.txt')\n",
    "        \n",
    "        print(f\"Loaded data: Train shapes X:{X_train.shape}, y:{y_train.shape}, Test shapes X:{X_test.shape}, y:{y_test.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Generating synthetic data for demonstration...\")\n",
    "        \n",
    "        # Generate synthetic data if file loading fails\n",
    "        # This is just for demonstration purposes\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # 78 training samples (26 letters x 3 fonts)\n",
    "        X_train = np.random.rand(78, 14) * 20\n",
    "        y_train = np.repeat(np.arange(26), 3)\n",
    "        \n",
    "        # 78 test samples (26 letters x 3 fonts)\n",
    "        X_test = np.random.rand(78, 14) * 20\n",
    "        y_test = np.repeat(np.arange(26), 3)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Main function to run experiments\n",
    "\n",
    "def experiment_mlp(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Run different MLP experiments.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with MLP results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Basic MLP with one hidden layer\n",
    "    print(\"  1.1 Testing basic MLP architecture...\")\n",
    "    mlp_basic = MLPClassifier(\n",
    "        hidden_layer_sizes=(28,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.0001,\n",
    "        batch_size='auto',\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    mlp_basic.fit(X_train, y_train)\n",
    "    y_pred = mlp_basic.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    Basic MLP accuracy: {accuracy:.4f}\")\n",
    "    results['basic'] = {\n",
    "        'model': mlp_basic,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 2. MLP with two hidden layers\n",
    "    print(\"  1.2 Testing two-layer MLP architecture...\")\n",
    "    mlp_two_layer = MLPClassifier(\n",
    "        hidden_layer_sizes=(28, 14),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.0001,\n",
    "        batch_size='auto',\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    mlp_two_layer.fit(X_train, y_train)\n",
    "    y_pred = mlp_two_layer.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    Two-layer MLP accuracy: {accuracy:.4f}\")\n",
    "    results['two_layer'] = {\n",
    "        'model': mlp_two_layer,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 3. MLP with regularization\n",
    "    print(\"  1.3 Testing MLP with regularization...\")\n",
    "    mlp_reg = MLPClassifier(\n",
    "        hidden_layer_sizes=(28, 14),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.01,  # L2 regularization\n",
    "        batch_size='auto',\n",
    "        learning_rate='adaptive',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    mlp_reg.fit(X_train, y_train)\n",
    "    y_pred = mlp_reg.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    MLP with regularization accuracy: {accuracy:.4f}\")\n",
    "    results['regularized'] = {'model': mlp_reg,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 4. Find best MLP parameters using grid search\n",
    "    print(\"  1.4 Finding optimal MLP parameters with grid search...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(28,), (28, 14), (56,), (56, 28)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        MLPClassifier(max_iter=1000, random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_mlp = grid_search.best_estimator_\n",
    "    y_pred = best_mlp.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    Optimal MLP accuracy: {accuracy:.4f}\")\n",
    "    print(f\"    Best MLP parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    results['optimal'] = {\n",
    "        'model': best_mlp,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def experiment_rbf(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Run different RBF Network experiments.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with RBF results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Basic RBF\n",
    "    print(\"  2.1 Testing basic RBF Network...\")\n",
    "    rbf_basic = RBFNetwork(\n",
    "        n_centers=52,  # 2 centers per class\n",
    "        sigma=1.0,\n",
    "        learning_rate=0.01,\n",
    "        epochs=100,\n",
    "        batch_size=10,\n",
    "        initialization='random'\n",
    "    )\n",
    "    \n",
    "    rbf_basic.fit(X_train, y_train)\n",
    "    y_pred = rbf_basic.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    Basic RBF accuracy: {accuracy:.4f}\")\n",
    "    results['basic'] = {\n",
    "        'model': rbf_basic,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 2. RBF with K-means initialization\n",
    "    print(\"  2.2 Testing RBF with K-means initialization...\")\n",
    "    rbf_kmeans = RBFNetwork(\n",
    "        n_centers=52,\n",
    "        sigma=1.0,\n",
    "        learning_rate=0.01,\n",
    "        epochs=100,\n",
    "        batch_size=10,\n",
    "        initialization='kmeans'\n",
    "    )\n",
    "    \n",
    "    rbf_kmeans.fit(X_train, y_train)\n",
    "    y_pred = rbf_kmeans.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    RBF with K-means initialization accuracy: {accuracy:.4f}\")\n",
    "    results['kmeans_init'] = {\n",
    "        'model': rbf_kmeans,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 3. RBF with per-class center initialization\n",
    "    print(\"  2.3 Testing RBF with per-class initialization...\")\n",
    "    rbf_per_class = RBFNetwork(\n",
    "        n_centers=52,  # 2 centers per class\n",
    "        sigma=1.0,\n",
    "        learning_rate=0.01,\n",
    "        epochs=100,\n",
    "        batch_size=10,\n",
    "        initialization='per_class'\n",
    "    )\n",
    "    \n",
    "    rbf_per_class.fit(X_train, y_train)\n",
    "    y_pred = rbf_per_class.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    RBF with per-class initialization accuracy: {accuracy:.4f}\")\n",
    "    results['per_class_init'] = {\n",
    "        'model': rbf_per_class,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 4. RBF with adaptive centers (two-phase learning)\n",
    "    print(\"  2.4 Testing RBF with adaptive centers...\")\n",
    "    rbf_adaptive = RBFNetwork(\n",
    "        n_centers=52,\n",
    "        sigma=1.0,\n",
    "        learning_rate=0.01,\n",
    "        epochs=100,\n",
    "        batch_size=10,\n",
    "        initialization='kmeans',\n",
    "        center_learning_rate=0.001  # Enable center adaptation\n",
    "    )\n",
    "    \n",
    "    rbf_adaptive.fit(X_train, y_train)\n",
    "    y_pred = rbf_adaptive.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    RBF with adaptive centers accuracy: {accuracy:.4f}\")\n",
    "    results['adaptive'] = {\n",
    "        'model': rbf_adaptive,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 5. Experiment with different numbers of centers\n",
    "    print(\"  2.5 Testing different numbers of RBF centers...\")\n",
    "    center_counts = [26, 52, 78]  # 1x, 2x, 3x the number of classes\n",
    "    for n_centers in center_counts:\n",
    "       \n",
    "        rbf = RBFNetwork(\n",
    "            n_centers=n_centers,\n",
    "            sigma=1.0,\n",
    "            learning_rate=0.01,\n",
    "            epochs=100,\n",
    "            batch_size=10,\n",
    "            initialization='kmeans'\n",
    "        )\n",
    "        \n",
    "        rbf.fit(X_train, y_train)\n",
    "        y_pred = rbf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"    RBF with {n_centers} centers accuracy: {accuracy:.4f}\")\n",
    "        results[f'centers_{n_centers}'] = {\n",
    "            'model': rbf,\n",
    "            'accuracy': accuracy,\n",
    "            'y_pred': y_pred,\n",
    "            'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def experiment_svm(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Run different SVM experiments.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with SVM results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Linear SVM\n",
    "    print(\"  3.1 Testing Linear SVM...\")\n",
    "    svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "    \n",
    "    svm_linear.fit(X_train, y_train)\n",
    "    y_pred = svm_linear.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    Linear SVM accuracy: {accuracy:.4f}\")\n",
    "    results['linear'] = {\n",
    "        'model': svm_linear,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 2. RBF SVM\n",
    "    print(\"  3.2 Testing RBF SVM...\")\n",
    "    svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "    \n",
    "    svm_rbf.fit(X_train, y_train)\n",
    "    y_pred = svm_rbf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    RBF SVM accuracy: {accuracy:.4f}\")\n",
    "    results['rbf'] = {\n",
    "        'model': svm_rbf,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 3. SVM with different C values\n",
    "    print(\"  3.3 Testing SVM with different C values...\")\n",
    "    c_values = [0.1, 1.0, 10.0, 100.0]\n",
    "    for c in c_values:\n",
    "        svm = SVC(kernel='rbf', C=c, gamma='scale', random_state=42)\n",
    "        \n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"    SVM with C={c} accuracy: {accuracy:.4f}\")\n",
    "        results[f'c_{c}'] = {\n",
    "            'model': svm,\n",
    "            'accuracy': accuracy,\n",
    "            'y_pred': y_pred,\n",
    "            'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "        }\n",
    "    \n",
    "    # 4. One-vs-Rest Strategy\n",
    "    print(\"  3.4 Testing One-vs-Rest SVM...\")\n",
    "    ovr_svm = OneVsRestClassifier(SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42))\n",
    "    \n",
    "    ovr_svm.fit(X_train, y_train)\n",
    "    y_pred = ovr_svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    One-vs-Rest SVM accuracy: {accuracy:.4f}\")\n",
    "    results['ovr'] = {\n",
    "        'model': ovr_svm,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 5. One-vs-One Strategy\n",
    "    print(\"  3.5 Testing One-vs-One SVM...\")\n",
    "    ovo_svm = OneVsOneClassifier(SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42))\n",
    "    \n",
    "    ovo_svm.fit(X_train, y_train)\n",
    "    y_pred = ovo_svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    One-vs-One SVM accuracy: {accuracy:.4f}\")\n",
    "    results['ovo'] = {\n",
    "        'model': ovo_svm,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    # 6. Grid search for best SVM parameters\n",
    "    print(\"  3.6 Finding optimal SVM parameters with grid search...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'kernel': ['rbf', 'poly'],\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        SVC(random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    y_pred = best_svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"    Optimal SVM accuracy: {accuracy:.4f}\")\n",
    "    print(f\"    Best SVM parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    results['optimal'] = {\n",
    "        'model': best_svm,\n",
    "        'accuracy': accuracy,\n",
    "        'y_pred': y_pred,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_models(results):\n",
    "    \"\"\"\n",
    "    Compare all models and visualize results.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary with results of all experiments\n",
    "    \"\"\"\n",
    "    # 1. Create a summary table of accuracies\n",
    "    print(\"\\nModel Accuracy Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<30} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    accuracies = {}\n",
    "    \n",
    "    for model_type, model_results in results.items():\n",
    "        for variant, variant_results in model_results.items():\n",
    "            model_name = f\"{model_type} ({variant})\"\n",
    "            accuracy = variant_results['accuracy']\n",
    "            accuracies[model_name] = accuracy\n",
    "            print(f\"{model_name:<30} {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 2. Visualize accuracies\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    models = list(accuracies.keys())\n",
    "    accs = list(accuracies.values())\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    sorted_indices = np.argsort(accs)[::-1]  # Descending order\n",
    "    sorted_models = [models[i] for i in sorted_indices]\n",
    "    sorted_accs = [accs[i] for i in sorted_indices]\n",
    "    \n",
    "    plt.bar(sorted_models, sorted_accs, color='skyblue')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Find the best model overall\n",
    "    best_model_name = sorted_models[0]\n",
    "    best_accuracy = sorted_accs[0]\n",
    "    print(f\"\\nBest Model: {best_model_name} with accuracy {best_accuracy:.4f}\")\n",
    "    \n",
    "    # 4. Visualize confusion matrix for best model\n",
    "    model_type, variant = best_model_name.split(' (')\n",
    "    variant = variant.rstrip(')')\n",
    "    \n",
    "    best_results = results[model_type][variant]\n",
    "    y_true = results[model_type][variant]['y_pred']  # Assuming same test set for all\n",
    "    y_pred = best_results['y_pred']\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('best_model_confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Visualize classification report for best model\n",
    "    report = best_results['report']\n",
    "    \n",
    "    # Extract metrics per class\n",
    "    class_precision = [report[str(i)]['precision'] for i in range(26)]\n",
    "    class_recall = [report[str(i)]['recall'] for i in range(26)]\n",
    "    # Continuing from line 741\n",
    "    class_f1 = [report[str(i)]['f1-score'] for i in range(26)]\n",
    "    \n",
    "    # Create dataframe for visualization\n",
    "    class_labels = [chr(65 + i) for i in range(26)]  # A-Z\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Class': class_labels,\n",
    "        'Precision': class_precision,\n",
    "        'Recall': class_recall,\n",
    "        'F1-Score': class_f1\n",
    "    })\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    metrics_df.plot(x='Class', y=['Precision', 'Recall', 'F1-Score'], kind='bar', figsize=(12, 6))\n",
    "    plt.title(f'Classification Metrics by Class - {best_model_name}')\n",
    "    plt.xlabel('Character Class')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('best_model_class_metrics.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Compare best models of each type\n",
    "    best_models = {}\n",
    "    for model_type, model_results in results.items():\n",
    "        best_variant = max(model_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "        best_models[model_type] = best_variant[1]\n",
    "    \n",
    "    print(\"\\nBest Model of Each Type:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model Type':<15} {'Best Variant':<15} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model_type, model_results in best_models.items():\n",
    "        variant = next((k for k, v in results[model_type].items() if v == model_results), \"\")\n",
    "        print(f\"{model_type:<15} {variant:<15} {model_results['accuracy']:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 7. Create an ensemble of the best models\n",
    "    print(\"\\n5. Creating an ensemble of the best models...\")\n",
    "    \n",
    "    # Get best model instances\n",
    "    best_mlp = best_models['MLP']['model']\n",
    "    best_rbf = best_models['RBF']['model']\n",
    "    best_svm = best_models['SVM']['model']\n",
    "    \n",
    "    # Create a voting classifier\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('mlp', best_mlp),\n",
    "            ('rbf', best_rbf),\n",
    "            ('svm', best_svm)\n",
    "        ],\n",
    "        voting='hard'\n",
    "    )\n",
    "    \n",
    "    # Need to refit since we already fit the individual models\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    y_pred_ensemble = ensemble.predict(X_test)\n",
    "    accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "    \n",
    "    print(f\"Ensemble model accuracy: {accuracy_ensemble:.4f}\")\n",
    "    \n",
    "    # Compare with best individual model\n",
    "    if accuracy_ensemble > best_accuracy:\n",
    "        print(f\"Ensemble outperforms best individual model by {accuracy_ensemble - best_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(f\"Best individual model outperforms ensemble by {best_accuracy - accuracy_ensemble:.4f}\")\n",
    "    \n",
    "    # 8. Additional visualizations\n",
    "    \n",
    "    # Visualize learning curves for best MLP\n",
    "    if 'MLP' in best_models:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(best_mlp.loss_curve_)\n",
    "        plt.title('MLP Learning Curve')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.savefig('mlp_learning_curve.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Visualize decision boundaries (only if 2D features available or using PCA)\n",
    "    # Not implemented here since we have 14-dimensional data\n",
    "    \n",
    "    return {\n",
    "        'accuracies': accuracies,\n",
    "        'best_model_name': best_model_name,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_models': best_models,\n",
    "        'ensemble_accuracy': accuracy_ensemble\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c92361a-1374-49e5-9d86-5ce85997b0ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Font Character Recognition Experiment\n",
      "===========================================\n",
      "\n",
      "Loading data...\n",
      "Loaded data: Train shapes X:(78, 14), y:(78,), Test shapes X:(78, 14), y:(78,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function to run the entire experiment pipeline.\n",
    "\"\"\"\n",
    "print(\"Multi-Font Character Recognition Experiment\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "# 1. Load and prepare data\n",
    "print(\"\\nLoading data...\")\n",
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03abc2e-4cc2-41b3-86a1-43b0596a6cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 78 samples, 14 features\n",
      "Test set: 78 samples, 14 features\n",
      "Number of classes: 26\n",
      "\n",
      "Exploring data...\n"
     ]
    }
   ],
   "source": [
    "# Convert class indices to letters for better readability\n",
    "class_names = [chr(65 + i) for i in range(26)]  # A-Z\n",
    "\n",
    "# 2. Display data information\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# 3. Explore data\n",
    "print(\"\\nExploring data...\")\n",
    "\n",
    "# Class distribution\n",
    "train_class_counts = np.bincount(y_train, minlength=26)\n",
    "test_class_counts = np.bincount(y_test, minlength=26)\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "x = np.arange(26)\n",
    "\n",
    "plt.bar(x - bar_width/2, train_class_counts, bar_width, label='Train')\n",
    "plt.bar(x + bar_width/2, test_class_counts, bar_width, label='Test')\n",
    "plt.xlabel('Character Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training and Test Sets')\n",
    "plt.xticks(x, class_names)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cdd2be7-9cc6-4b7c-bc51-626fd20e018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiments with MLP, RBF, and SVM for multi-font character recognition...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run experiments with MLP, RBF, and SVM models.\n",
    "\n",
    "Args:\n",
    "    X_train: Training features\n",
    "    y_train: Training labels\n",
    "    X_test: Test features\n",
    "    y_test: Test labels\n",
    "\n",
    "Returns:\n",
    "    Dictionary with results of all experiments\n",
    "\"\"\"\n",
    "results = {}\n",
    "\n",
    "print(\"Starting experiments with MLP, RBF, and SVM for multi-font character recognition...\")\n",
    "\n",
    "# Create copies of the data for potential different preprocessing\n",
    "X_train_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_test_scaled = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df21318-05d6-4d14-be65-3242b3e77f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Running MLP experiments...\n",
      "  1.1 Testing basic MLP architecture...\n",
      "    Basic MLP accuracy: 0.8462\n",
      "  1.2 Testing two-layer MLP architecture...\n",
      "    Two-layer MLP accuracy: 0.8590\n",
      "  1.3 Testing MLP with regularization...\n",
      "    MLP with regularization accuracy: 0.8590\n",
      "  1.4 Finding optimal MLP parameters with grid search...\n",
      "    Optimal MLP accuracy: 0.8846\n",
      "    Best MLP parameters: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (56,), 'learning_rate': 'constant'}\n"
     ]
    }
   ],
   "source": [
    "# 1. MLP Experiments\n",
    "print(\"\\n1. Running MLP experiments...\")\n",
    "mlp_results = experiment_mlp(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "results['MLP'] = mlp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ad9303-ac29-4672-a709-8777f66471d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Running RBF Network experiments...\n",
      "  2.1 Testing basic RBF Network...\n",
      "    Basic RBF accuracy: 0.8846\n",
      "  2.2 Testing RBF with K-means initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\NM TRADERS\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\NM TRADERS\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\NM TRADERS\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RBF with K-means initialization accuracy: 0.8718\n",
      "  2.3 Testing RBF with per-class initialization...\n",
      "    RBF with per-class initialization accuracy: 0.8462\n",
      "  2.4 Testing RBF with adaptive centers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RBF with adaptive centers accuracy: 0.8718\n",
      "  2.5 Testing different numbers of RBF centers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RBF with 26 centers accuracy: 0.7564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RBF with 52 centers accuracy: 0.8718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\NM TRADERS\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (70) found smaller than n_clusters (78). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    RBF with 78 centers accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "# 2. RBF Experiments\n",
    "print(\"\\n2. Running RBF Network experiments...\")\n",
    "rbf_results = experiment_rbf(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "results['RBF'] = rbf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "507b8519-ac73-4376-99fc-a8b98a74c2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Running SVM experiments...\n",
      "  3.1 Testing Linear SVM...\n",
      "    Linear SVM accuracy: 0.8718\n",
      "  3.2 Testing RBF SVM...\n",
      "    RBF SVM accuracy: 0.8333\n",
      "  3.3 Testing SVM with different C values...\n",
      "    SVM with C=0.1 accuracy: 0.8077\n",
      "    SVM with C=1.0 accuracy: 0.8333\n",
      "    SVM with C=10.0 accuracy: 0.8590\n",
      "    SVM with C=100.0 accuracy: 0.8718\n",
      "  3.4 Testing One-vs-Rest SVM...\n",
      "    One-vs-Rest SVM accuracy: 0.8462\n",
      "  3.5 Testing One-vs-One SVM...\n",
      "    One-vs-One SVM accuracy: 0.8333\n",
      "  3.6 Finding optimal SVM parameters with grid search...\n",
      "    Optimal SVM accuracy: 0.8590\n",
      "    Best SVM parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# 3. SVM Experiments\n",
    "print(\"\\n3. Running SVM experiments...\")\n",
    "svm_results = experiment_svm(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "results['SVM'] = svm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c1c22c5-cd0a-4460-bf94-caf3481ebcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Comparing all models...\n",
      "\n",
      "Model Accuracy Comparison:\n",
      "------------------------------------------------------------\n",
      "Model                          Accuracy  \n",
      "------------------------------------------------------------\n",
      "MLP (basic)                    0.8462\n",
      "MLP (two_layer)                0.8590\n",
      "MLP (regularized)              0.8590\n",
      "MLP (optimal)                  0.8846\n",
      "RBF (basic)                    0.8846\n",
      "RBF (kmeans_init)              0.8718\n",
      "RBF (per_class_init)           0.8462\n",
      "RBF (adaptive)                 0.8718\n",
      "RBF (centers_26)               0.7564\n",
      "RBF (centers_52)               0.8718\n",
      "RBF (centers_78)               0.8718\n",
      "SVM (linear)                   0.8718\n",
      "SVM (rbf)                      0.8333\n",
      "SVM (c_0.1)                    0.8077\n",
      "SVM (c_1.0)                    0.8333\n",
      "SVM (c_10.0)                   0.8590\n",
      "SVM (c_100.0)                  0.8718\n",
      "SVM (ovr)                      0.8462\n",
      "SVM (ovo)                      0.8333\n",
      "SVM (optimal)                  0.8590\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best Model: MLP (optimal) with accuracy 0.8846\n",
      "\n",
      "Best Model of Each Type:\n",
      "------------------------------------------------------------\n",
      "Model Type      Best Variant    Accuracy  \n",
      "------------------------------------------------------------\n",
      "MLP             optimal         0.8846\n",
      "RBF             basic           0.8846\n",
      "SVM             linear          0.8718\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. Creating an ensemble of the best models...\n",
      "Ensemble model accuracy: 0.8846\n",
      "Best individual model outperforms ensemble by 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracies': {'MLP (basic)': 0.8461538461538461,\n",
       "  'MLP (two_layer)': 0.8589743589743589,\n",
       "  'MLP (regularized)': 0.8589743589743589,\n",
       "  'MLP (optimal)': 0.8846153846153846,\n",
       "  'RBF (basic)': 0.8846153846153846,\n",
       "  'RBF (kmeans_init)': 0.8717948717948718,\n",
       "  'RBF (per_class_init)': 0.8461538461538461,\n",
       "  'RBF (adaptive)': 0.8717948717948718,\n",
       "  'RBF (centers_26)': 0.7564102564102564,\n",
       "  'RBF (centers_52)': 0.8717948717948718,\n",
       "  'RBF (centers_78)': 0.8717948717948718,\n",
       "  'SVM (linear)': 0.8717948717948718,\n",
       "  'SVM (rbf)': 0.8333333333333334,\n",
       "  'SVM (c_0.1)': 0.8076923076923077,\n",
       "  'SVM (c_1.0)': 0.8333333333333334,\n",
       "  'SVM (c_10.0)': 0.8589743589743589,\n",
       "  'SVM (c_100.0)': 0.8717948717948718,\n",
       "  'SVM (ovr)': 0.8461538461538461,\n",
       "  'SVM (ovo)': 0.8333333333333334,\n",
       "  'SVM (optimal)': 0.8589743589743589},\n",
       " 'best_model_name': 'MLP (optimal)',\n",
       " 'best_accuracy': 0.8846153846153846,\n",
       " 'best_models': {'MLP': {'model': MLPClassifier(hidden_layer_sizes=(56,), max_iter=1000, random_state=42),\n",
       "   'accuracy': 0.8846153846153846,\n",
       "   'y_pred': array([ 0,  0,  7,  1,  1,  1,  2,  2,  2,  3,  3,  3,  4,  4,  4,  5,  5,\n",
       "           5,  6,  6,  6,  7,  7,  7,  8,  8,  8,  8,  9,  9, 10, 10, 24, 11,\n",
       "          11, 11, 12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15, 23, 16,  6,\n",
       "          17, 17, 17, 18, 18,  2, 19, 19, 19, 20, 20,  8, 21, 21, 21, 22, 22,\n",
       "          24, 23, 23, 23, 24, 24,  9, 25, 25, 25], dtype=int64),\n",
       "   'report': {'0': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '2': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '3': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '4': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '5': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '6': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '7': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '8': {'precision': 0.6, 'recall': 1.0, 'f1-score': 0.75, 'support': 3.0},\n",
       "    '9': {'precision': 0.6666666666666666,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.6666666666666666,\n",
       "     'support': 3.0},\n",
       "    '10': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '11': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '12': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '13': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '14': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '15': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '16': {'precision': 1.0,\n",
       "     'recall': 0.3333333333333333,\n",
       "     'f1-score': 0.5,\n",
       "     'support': 3.0},\n",
       "    '17': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '18': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '19': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '20': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '21': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '22': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '23': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '24': {'precision': 0.5,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.5714285714285714,\n",
       "     'support': 3.0},\n",
       "    '25': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    'accuracy': 0.8846153846153846,\n",
       "    'macro avg': {'precision': 0.9141025641025641,\n",
       "     'recall': 0.8846153846153846,\n",
       "     'f1-score': 0.8814102564102565,\n",
       "     'support': 78.0},\n",
       "    'weighted avg': {'precision': 0.9141025641025641,\n",
       "     'recall': 0.8846153846153846,\n",
       "     'f1-score': 0.8814102564102563,\n",
       "     'support': 78.0}},\n",
       "   'best_params': {'activation': 'relu',\n",
       "    'alpha': 0.0001,\n",
       "    'hidden_layer_sizes': (56,),\n",
       "    'learning_rate': 'constant'}},\n",
       "  'RBF': {'model': RBFNetwork(initialization='random'),\n",
       "   'accuracy': 0.8846153846153846,\n",
       "   'y_pred': array([ 0,  0,  7,  1,  1,  1,  2,  2, 14,  3,  3,  3,  4,  4,  4,  5,  5,\n",
       "           5,  6,  6,  6,  7,  7,  7,  8,  8,  8,  8,  9,  9, 10, 10, 10, 11,\n",
       "          11, 11, 12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15, 23, 16, 16,\n",
       "          15, 17, 17, 18, 18, 24, 19, 19, 19, 20, 20, 12, 21, 21, 21, 22, 22,\n",
       "          20, 23, 23, 23, 24, 24,  9, 25, 25, 25], dtype=int64),\n",
       "   'report': {'0': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '2': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '3': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '4': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '5': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '6': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '7': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '8': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '9': {'precision': 0.6666666666666666,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.6666666666666666,\n",
       "     'support': 3.0},\n",
       "    '10': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '11': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '12': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '13': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '14': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '15': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '16': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '17': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '18': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '19': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '20': {'precision': 0.6666666666666666,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.6666666666666666,\n",
       "     'support': 3.0},\n",
       "    '21': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '22': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '23': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '24': {'precision': 0.6666666666666666,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.6666666666666666,\n",
       "     'support': 3.0},\n",
       "    '25': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    'accuracy': 0.8846153846153846,\n",
       "    'macro avg': {'precision': 0.9038461538461539,\n",
       "     'recall': 0.8846153846153846,\n",
       "     'f1-score': 0.8824175824175826,\n",
       "     'support': 78.0},\n",
       "    'weighted avg': {'precision': 0.9038461538461539,\n",
       "     'recall': 0.8846153846153846,\n",
       "     'f1-score': 0.8824175824175823,\n",
       "     'support': 78.0}}},\n",
       "  'SVM': {'model': SVC(kernel='linear', random_state=42),\n",
       "   'accuracy': 0.8717948717948718,\n",
       "   'y_pred': array([ 0,  0,  2,  1,  1,  1,  2,  2, 14,  3,  3,  3,  4,  4,  4,  5,  5,\n",
       "           5,  6,  6,  6,  7,  7,  7,  8,  8,  8,  8,  9,  9, 10, 10, 24, 11,\n",
       "          11, 11, 12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15, 23, 16, 16,\n",
       "          15, 17, 17, 18, 18, 24, 19, 19, 19, 20, 20, 12, 21, 21, 21, 22, 22,\n",
       "          20, 23, 23, 23, 24, 24,  9, 25, 25, 25], dtype=int64),\n",
       "   'report': {'0': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '2': {'precision': 0.6666666666666666,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.6666666666666666,\n",
       "     'support': 3.0},\n",
       "    '3': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '4': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '5': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '6': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '7': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '8': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '9': {'precision': 0.6666666666666666,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.6666666666666666,\n",
       "     'support': 3.0},\n",
       "    '10': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '11': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '12': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '13': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '14': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '15': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '16': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '17': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '18': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '19': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '20': {'precision': 0.6666666666666666,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.6666666666666666,\n",
       "     'support': 3.0},\n",
       "    '21': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    '22': {'precision': 1.0,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.8,\n",
       "     'support': 3.0},\n",
       "    '23': {'precision': 0.75,\n",
       "     'recall': 1.0,\n",
       "     'f1-score': 0.8571428571428571,\n",
       "     'support': 3.0},\n",
       "    '24': {'precision': 0.5,\n",
       "     'recall': 0.6666666666666666,\n",
       "     'f1-score': 0.5714285714285714,\n",
       "     'support': 3.0},\n",
       "    '25': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0},\n",
       "    'accuracy': 0.8717948717948718,\n",
       "    'macro avg': {'precision': 0.8942307692307693,\n",
       "     'recall': 0.8717948717948718,\n",
       "     'f1-score': 0.8714285714285714,\n",
       "     'support': 78.0},\n",
       "    'weighted avg': {'precision': 0.8942307692307693,\n",
       "     'recall': 0.8717948717948718,\n",
       "     'f1-score': 0.8714285714285713,\n",
       "     'support': 78.0}}}},\n",
       " 'ensemble_accuracy': 0.8846153846153846}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Compare all models\n",
    "print(\"\\n4. Comparing all models...\")\n",
    "compare_models(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f0fd2-556d-43bc-8817-09be2bc78e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d95fa-2193-4bff-8375-cedefb8ea20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
